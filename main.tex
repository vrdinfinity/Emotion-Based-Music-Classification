\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{graphics}
\graphicspath{ {./images/} }
\usepackage{multicol}

% width,height
\usepackage[a4paper, total={7in, 10in}]{geometry}

\title{Emotion-Based Music Classification}
\author{Vaibhav Raj Dikshit.P}
\date{\today}

\begin{document}

    \input{titlepage}
    
    \begin{center}
        \section*{Emotion-Based Music Classification}
    \end{center}
\setlength{\columnsep}{1.0cm}
    \large
    \section{Summary}
    Automatic Emotion-Based Music Classification is a method for classifying music tracks based on the emotions they evoke in listeners. This technique can be used to support intelligent Internet of Things (IoT) applications, such as personal assistants, by providing them with the ability to select music that matches a user's current mood. The classification is typically done using machine learning algorithms that analyze the audio features of the music, such as tempo, rhythm, and harmony, and compare them to a dataset of pre-labeled music tracks. This method can improve the user experience of IoT devices by providing personalized music recommendations and creating a more emotionally engaging environment.\\
    
    This paper discusses using automatic emotion-based music classification to support intelligent Internet of Things (IoT) applications. The proposal is a method for classifying music into different emotional categories (e.g. happy, sad, etc.) using machine learning techniques, and suggest that this classification could be used to improve the user experience in IoT devices, such as home assistants or smart speakers. The goal is to make the devices responsive to the emotional state of the user and provide music that matches their current mood.\\
    
   The aim of the paper  is to propose and evaluate a method for automatically classifying music into different emotional categories using machine learning techniques, and to explore the potential use of this classification in improving the user experience in Internet of Things (IoT) devices. The authors suggest that by making the devices responsive to the emotional state of the user and providing music that matches their current mood, the overall user experience can be enhanced. 




    
\begin{multicols}{1}    
    \section*{Ideas from the author}
    %our understanding on what the authors have contributed or raised as comments regarding the topic.
    
The authors of the paper "Automatic Emotion-Based Music Classification for Supporting Intelligent IoT Applications" propose the following ideas:

    
    \begin{enumerate}
        \item Using machine learning techniques to classify music into different emotional categories (e.g. happy, sad, etc.) as a way to improve the user experience in IoT devices, such as home assistants or smart speakers.
        \item Making the IoT devices responsive to the emotional state of the user by providing music that matches their current mood.
        \item Conducting an experiment to evaluate the proposed method using a dataset of songs, with promising results.
        \item Using the emotion-based music classification in various IoT applications such as smart home, smart entertainment and smart car systems.
        \item The proposed method can also be useful in areas such as music therapy, and personalizing music recommendations.
        \item The authors also suggested future work in the area of improve the emotion recognition by using multimodal data such as speech and facial expressions.
    \end{enumerate}
    

    \section*{My views about this paper}
    %Your views on the topic, along with what you envision the future of AI would be in the topic discussed in the paper.
   The concept of using automatic emotion-based music classification in IoT devices is interesting and has the potential to greatly enhance the user experience. Being able to match the music to the user's emotional state, such as providing calming music when the user is feeling stressed, or upbeat music when they are feeling happy, could greatly improve the overall user experience. Additionally, the idea of using multimodal data such as speech and facial expressions to improve the emotion recognition is also promising. However, the effectiveness of this approach will depend on the accuracy of the emotion classification and the ability to effectively integrate it into IoT devices.\\
    

    
    \section*{Agreement, Pitfalls and Fallacies}
    %State what and why do you agree and disagree with the views presented in the paper. You can present this as a table, with the list of your agreements and disagreements on the key aspects of the paper (For example, you might feel that the conclusions drawn by the authors towards the claims made in the research might not agree to your views. Or else, you very well resonate with the thoughts of the author. In either case, you must list the same in brief phrases, and state the reason for each agreement/disagreement. The list can have as many as your understanding permits).
    
    \begin{itemize}
     The agreement would likely outline the specific tasks and responsibilities of the parties involved in the project, as well as any intellectual property rights or confidentiality agreements.\\
     
     There are several potential pitfalls that may be encountered when implementing automatic emotion-based music classification for supporting intelligent IoT applications, such as:

Limited dataset: The accuracy of the emotion classification model may be limited by the size and diversity of the dataset used for training.

Subjectivity of emotion: Emotions are subjective and can vary between individuals, making it difficult to accurately classify music into specific emotional categories.

Complexity of music: Music can have multiple layers of emotion and meaning, making it difficult for a model to accurately classify the overall emotional tone of a song.

Data bias: If the data used to train the model is biased, it can lead to inaccurate or unfair classifications.

Overfitting: Overfitting can occur when a model is trained too well on the training data, and is not able to generalize well to new data.

Inaccurate real-time data input: Emotion recognition depends on the real-time data input, which can be inaccurately captured by the devices, leading to poor classifications.

Privacy concerns: The use of personal data and the storage and processing of it, could raise privacy concerns, which must be addressed in the agreement.\\ 

 In addition to the potential pitfalls, there are also several fallacies that may be encountered when implementing automatic emotion-based music classification for supporting intelligent IoT applications. Some of them are:

False assumptions: Assuming that a certain type of music will always evoke a specific emotion in all listeners, can lead to inaccurate classifications.

Overgeneralization: Overgeneralizing the results of the model, such as assuming that it applies to all types of music or all listeners, can lead to inaccurate conclusions.

Ignoring context: Not considering the context in which the music is being listened to, can lead to inaccurate classifications.

Unrealistic expectations: Expecting the model to have 100% accuracy, can lead to disappointment with its performance.

Confusing correlation with causality: Correlating the emotion of the music with the emotion of the listener without understanding the underlying causes can lead to inaccurate conclusions.

Not considering other factors: Not considering other factors that may influence the listener's emotional response to the music, such as personal experiences or cultural background, can lead to inaccurate classifications.

Not updating model: Not updating the model on regular basis with new data and new techniques can lead to poor performance and less accuracy.
    \end{itemize}
    
\end{multicols}
\end{document}

